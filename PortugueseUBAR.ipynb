{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djMzdUW4Aj56",
    "outputId": "9129d7a2-40b2-447f-f976-6013c6af97c1"
   },
   "outputs": [],
   "source": [
    "#!wget --continue http://www.lrc.ic.unicamp.br/~ferraroni/datasets/adrenaline/conversations.zip\n",
    "#!unzip conversations.zip >> log.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClYe2x6hAKlP"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet \"scikit-learn\" \"datasets\" \"scipy\" \"torchmetrics>=0.3\" \"transformers==4.9.0\" \"torch>=1.9\" \"pytorch-lightning>=1.3\" \"huggingface-hub==0.0.12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K-rdLMtL6gg1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K-rdLMtL6gg1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K-rdLMtL6gg1"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sg73hPgLAGt7"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4ug5r-gOZCuj"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(\"clear_threads/*_*.tsv\")\n",
    "\n",
    "frames = []\n",
    "counter = 1\n",
    "\n",
    "for file in tqdm.notebook.tqdm(files):\n",
    "    counter += 1\n",
    "    df = pd.read_csv(file, delimiter='\\t', quoting=3, header=None, names=[\"timestamp\", \"id\", \"text\"])\n",
    "    df.drop(columns=['timestamp'], inplace=True)\n",
    "    df['reply'] = df['text'].shift(-1)\n",
    "    df = df.iloc[:-1]\n",
    "    os.remove(file)\n",
    "    frames.append(df)\n",
    "    if counter % 1000 == 0:\n",
    "        out = pd.concat(frames, axis=0, ignore_index=True)\n",
    "        out.dropna(axis=0, inplace=True)\n",
    "        frames = []\n",
    "        out.to_parquet(file.replace('.tsv', '.parquet'))\n",
    "        del out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, validation_files = train_test_split(glob.glob(\"clear_threads/11*.parquet\"), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-62766e55e41dd593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/jader/.cache/huggingface/datasets/parquet/default-62766e55e41dd593/0.0.0/03dda9603b6ba3760d9d286684a3d7d8ec00448c154f765795485acd3229ecba...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/jader/.cache/huggingface/datasets/parquet/default-62766e55e41dd593/0.0.0/03dda9603b6ba3760d9d286684a3d7d8ec00448c154f765795485acd3229ecba. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c97f99e654159bad7eae44e811b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3201db14bc2945119b2c786aece9eb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"parquet\", data_files={\"train\": train_files, \"validation\": validation_files})\n",
    "datasets = datasets.filter(lambda x: x['text'] is not None and x['reply'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/members/hoguga.154856/</td>\n",
       "      <td>Opa.. Segue meus testes: &lt;url&gt; [email protected] &lt;/url&gt; /1100 &lt;url&gt; img405.imageshack.us &lt;image&gt; &lt;/url&gt; &lt;url&gt; [email protected] &lt;/url&gt; Stock &lt;url&gt; img405.imageshack.us &lt;image&gt; &lt;/url&gt; &lt;url&gt; img41.imageshack.us &lt;image&gt; &lt;/url&gt; &lt;url&gt; img694.imageshack.us &lt;image&gt; &lt;/url&gt; Claramente o teste da area 01 força a gpu e o restante testa o processador... &lt;emoji&gt; :D &lt;/emoji&gt; abraçoss :yes:</td>\n",
       "      <td>Do teste 2 ao 4 sua HD 5850 está com gargalo por causa da CPU, podia render mais, porém nada que deva atrapalhar, a não ser pelo fps mínimo que não tem como avaliar por esse bench. Falou, bom divertimento :yes:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/members/rntbarbosa.2499868/</td>\n",
       "      <td>O ping ficou melhor, ou continua alto? Após o reparo na TAR, a linha voltou a suportar 15mb? O ruido tá alto e o modem entrou em modo adaptativo, 25mb deve alinhar em ~27.xxx Kbps. Já corrigiram o problema? Tô a um mês tentando a substituição meu td5130v1 que uso a uns 4 anos, e agora começou a travar aleatoriamente, a oi mandou 3x um técnico na minha casa, mas ele não tinha modem novo no carro, até fiz uma reclamação na anatel sobre isso, deixei bem claro que o modem precisava ser trocado e que isso era responsabilidade da OI pois, no contrato o modem é cedido em regime de comodato ( &lt;url&gt; www.oi.com.br &lt;link&gt; &lt;/url&gt; ), e o que a OI fez?? Abriu outro reparo, e mandou mais um técnico sem modem. Só me resta reclamar no consumidor.gov. Notou se o wifi do pace é mais forte que o do tech? Não achei firmware genérico desse modem, acho que tem como extrair ele do modem por telnet. Se vc poder, extrai ele e posta aqui. Quanto tá seu snr e att? O segundo modem também veio sem firmware personalizado para a OI/fon? No sac não temos como solicitar essa mudança, só o técnico do fixo pode fazer essa troca, se tiver viabilidade, e se ele quiser, pq tem uns que não gostam de fazer esse procedimento, ou seja, depende vontade/pressa do técnico. O snr se manteve o mesmo nos dois, obrigado por postar esse comparativo. Descobriu se é a placa wifi do note que não tá suportando mais do que 2.2mb/s? Atualizou o driver da placa wifi?</td>\n",
       "      <td>Sim, ambos vieram sem FON, pelo que o técnico disse, eles são diferentes pois eram pra ser os modens de teste aqui na região, mas colocaram pra mim pois não tinham outros.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/members/rexbauru.222900/</td>\n",
       "      <td>olhei aqui e o watmam mostra temp máxima 90ºC temp alvo 75ºC , não tentei baixar a voltagem não.</td>\n",
       "      <td>Então realmente o perfil da fan na Sapphire é bem modesto, porque deixar a placa passar 4°C do target é dose... Falouws &lt;emoji&gt; :cool: &lt;/emoji&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/members/lu4414.2714338/</td>\n",
       "      <td>Não há problema nenhum nisso, isso tem a ver com a região. Locais ex-telesp que receberam a rede primeiro (na época do fusionado) tem um over maior e nas regiões novas eles controlam melhor direto na OLT. Isso de expansão tem a ver com concorrência. A meta da Vivo é (e de qualquer outra hehehe) é lucrar e a estrutura de rede deles se alinha com esse objetivo. Não compreendo o porque cabear só 1/3 de um prédio (será que eles não confiavam que a demanda seria maior?), não sei se é falha de planejamento ou se o plano é esse e tudo bem para eles não atender todos.... O que sei que deve crescer com o tempo - e talvez ajude nisso - é a pressão por boas opções de internet na hora de comprar/alugar um espaço, já passou da hora de construtoras e teles se conversarem melhor!</td>\n",
       "      <td>entendi. Porem algo de errado não está certo. hoje minha conexão está muito instável, com dificuldade de carregar algumas paginas que acesso para trabalho e com uma limitação de velocidade bem abaixo da contratada. nos testes que fiz hoje, a velocidade máxima alcançada foi de 220/120mb (meu plano é 300/150) por cabo de rede e com somente meu computador ativo na rede. reiniciei o modem, e não obtive nenhuma melhora de velocidade ou nos carregamentos de pagina. mudei meu cabo de rede para o modem da Claro/NET, e minha navegação ficou normal. o estranho é que mesmo com a velocidade dando 220mb, eu não conseguia abrir algumas paginas de forma correta, aonde fica carregando e demorando para carregar as paginas. não sei se a causa é porque a rede de fibra é nova e precisa de algum ajuste externo, ou é problema na instalação/modem.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/members/fgmirra.158941/</td>\n",
       "      <td>Estou fazendo uns testes com uma XFX RX480 gtr black edition, porém não é minha, é emprestada &lt;emoji&gt; :bua: &lt;/emoji&gt; , porém o proprietário está pensando em pegar uma GTX 1070/1080, talvez consiga deixa-la por aqui. &lt;emoji&gt; :notsure: &lt;/emoji&gt; O driver que estou usando é Crimson Edition 16.11.2 Hotfix não WHQL, deve ser por isso que aparece como não aprovado no 3dmark. Se puder, faça os testes com as mesmas configurações do _EagleMan_ nos mesmo jogos, pode postar de outros games também, é só mostrar a configuração usada para termos um parâmetro, no momento estou com poucos games aqui. &lt;emoji&gt; :joia: &lt;/emoji&gt; No shadow of mordor rodei o benchmark do game para ter uma ideia dos fps. Vou colocar um teste que fiz do gameplay gravado pelo fraps abaixo e como ficou os dados da placa pelo GPUZ, já adianto que usou bem mais que 6gb de vram em 1080p &lt;emoji&gt; ;) &lt;/emoji&gt; Difícil dizer, acho que se puder partir para uma GTX 1070 seria uma escolha melhor. &lt;emoji&gt; :hmm2: &lt;/emoji&gt; O cpu da uma segura mesmo, normal, vi que o ROTR é bem mais exigente em relação a cpu do que o Tomb Raider 2013, mas essa melhora nos fps mínimos foi muito bem vinda usando o win10. &lt;emoji&gt; :joia: &lt;/emoji&gt; Mesmo no win8.1 que tenho aqui, não chega estragar a experiencia no game. Outra coisa que deve complicar é a ram, 8gb está sendo pouco nessa leva de games que estão saindo. Como esperou esse tempo todo é melhor esperar os Zen/Sky/Kabylake mesmo e já parte para 32gb de ram &lt;emoji&gt; :mesa: &lt;/emoji&gt; pensando nos ports porcos e longividade da plataforma. É o que pretendo fazer para o ano que vem &lt;emoji&gt; :bat: &lt;/emoji&gt; Testes gravado pelo FRAPS do gameplay resolução 1920x1080 quality ultra ShadowOfMordor Frames: 13600 - Time: 180468ms - Avg: 75.360 - Min: 50 - Max: 100 Apesar dos FPS estarem ótimos a placa varia o uso da GPU, e os core clock/memory clock no talo. &lt;image&gt; resolução 2880x1620 quality ultra ShadowOfMordor Frames: 23438 - Time: 423891ms - Avg: 55.293 - Min: 42 - Max: 65 Agora aumentando a resolução &lt;emoji&gt; :olho: &lt;/emoji&gt; &lt;image&gt; vlw &lt;emoji&gt; :) &lt;/emoji&gt;</td>\n",
       "      <td>Amigo legal esse em, deixar na sua mão uma vga novinha e atual &lt;emoji&gt; :lindo: &lt;/emoji&gt; , aqui na cidade só tenho um amigo que joga no pc só que ele tem uma 750 ti 2gb, e só joga Dota 2 rsrsrrs Tenho postados fotos do gpu-z com o resultado de consumo dos jogos, vou continuar postando aqui, deve interessar aos amigos &lt;emoji&gt; :) &lt;/emoji&gt;   olhei no ebay e sites chineses (dx, ali, gear) isso foi a uns 45~60dias, na epoca tinha (e parece que ainda tem, não pesquisei) no ml por 100R$, tive sorte e na epoca tinha uma mulher vendendo por 70R$ paguei mais 12R$ de frete, veio em uma caixa como com garantia e tudo, eu acho que vc não deve contar com um possível receptor Bluetooth não, o receptor wiriless funciona muito bem, não tem porque lançar outro produto, lembrando que quando lançou esse receptor ele não era nada barato la no pais de origem eua.   kkkk nem vi que escrevi gatinhos. a M$ seria muito bondosa se oferecesse suporte a receptores Bluetooth de terceiros, um receptor desse e baratíssimo, o meu que comprei 2014 paguei 25R$, ta certo que não deve ser o modelo novo 4, mas receptor bluetooth normalmente é bem barato, ai a M$ perderia dinheiro pois não venderia mais nenhum receptor wirilless que ela mesma \"criou\"   cade as fotos jovem??? posta umas duas ai &lt;emoji&gt; :joia: &lt;/emoji&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/members/juniorcsar.2564514/</td>\n",
       "      <td>Muito estranho sua CX 430 não estar segurando a RX 470. Também tenho essa fonte, a minha RX 470 é a da Sapphire e quando tô jogando o clock sempre fica cravado em 1260MHz e nunca deu problema aqui.</td>\n",
       "      <td>Por causa disso que suspeito que seja problema com o exemplar dele, seja fonte ou VGA. RAM eu também não descarto, mas apresentar reset aleatório só pelo fato de ter trocado a VGA? Bastante improvável. Foi como eu disse: em termos de trabalhar na melhor eficiência energética possível, essa fonte não seria a ideal, em virtude do PC dele estar consumindo em torno de 365W, enquanto que o sweet spot dessa fonte giraria em torno dos 215W; porém, no que diz respeito à ter potência para suprir a demanda do PC como um todo, ela sem dúvida possui. O que me admira é que ele saiu de uma 260X, que nem de longe é econômica (TDP de 115W e eu acho que ela consome até mais do que isso), para outra que consome razoavelmente bem, porém não tanto à mais quanto a 260X anterior (TDP de 120W, podendo naturalmente superar esse valor mediante overclock ou aquecimento demasiado da GPU). Se for problema com a fonte, é algum defeito que surgiu com o passar do tempo (poeira agindo, umidade também, maresia, essas coisas), mas ainda acho difícil de ser. Como você mesmo suspeita, existe uma possibilidade de ser realmente superaquecimento, pois a 470, por ser open air e ter maiores dimensões, produz um volume maior de ar quente que é dissipado pela parte superior da VGA, e esse ar deve realmente estar sendo absorvido pela fan do cooler da CPU, devido à posição da sua instalação. A questão é que o &lt;url&gt; forum.adrenaline.com.br @flamenguistak &lt;/url&gt; alega que o PC reiniciou, ou seja, esse comportamento não é sintoma de superaquecimento da CPU, pois se fosse, o PC desligaria ao invés de reiniciar. Cable management praticamente não tem impacto na temperatura dos componentes do PC: &lt;mediaembed&gt; youtube &lt;/mediaembed&gt; Reduzir o clock reduz o aquecimento, é verdade, mas ele usou o MSI Afterburner para fazer uma curva custom da fan, então a temperatura iria cair de qualquer jeito. Portanto, diante das informações que temos disponíveis, desconfio que o problema seja relacionado ou à fonte (considerando a manifestação de um defeito qualquer que a impossibilitou a mesma de atingir a demanda exigida, ou então algo relacionado à idade dela, uma vez que as fontes perdem capacidade de fornecer a potência rotulada em função do envelhecimento dos componentes internos, mas não sei até que ponto isso afetou negativamente essa fonte, que já tem seus 3 anos de vida) ou à própria VGA (que talvez esteja instável para trabalhar com o clock de fábrica com o vcore/power limit stock; até o momento, são as únicas coisas que consigo pensar, até porque são 02:00 aqui e eu tô quase desmaiando em cima do teclado &lt;emoji&gt; :haha: &lt;/emoji&gt; ). Pra finalizar alguns benchmarks feitos em cima da minha 750 TI SC da EVGA: &lt;spoiler&gt; Spoiler: EVGA GTX 750 TI SC 2GB Benchmarks Valley Benchmark - Extreme HD: &lt;image&gt; Tomb Raider 2013 - Max Settings, SSAA 2X: &lt;image&gt; Rise of The Tomb Raider - Max Settings, SMAA: &lt;image&gt; &lt;/spoiler&gt; Dos que tenho com benchmark embutido, só faltou o Hitman Absolution, mas já o pus pra baixar &lt;emoji&gt; :joia: &lt;/emoji&gt; Vamos ver como a RX se sai rodando esses jogos quando ela chegar amanhã... &lt;emoji&gt; :isso: &lt;/emoji&gt; &lt;emoji&gt; :mr: &lt;/emoji&gt; &lt;emoji&gt; :magico: &lt;/emoji&gt; &lt;emoji&gt; :gaming: &lt;/emoji&gt; Pouco ganho, mas pelo menos teve e é o que importa. Mas como que está o gameplay? Zoado feito BF ou mais parecido com o DX11? &lt;emoji&gt; :hmm2: &lt;/emoji&gt; EDIT: EXTRA, EXTRA!! A Mamute abaixou o preço da 1050 TI! &lt;spoiler&gt; Spoiler &lt;image&gt; Antes era... &lt;image&gt; Nossa, que super redução de quase R$ 20 no preço à vista foi essa, meu Deus?! Tá barato demais!! #SQN! &lt;emoji&gt; :pff: &lt;/emoji&gt; &lt;emoji&gt; :bwahaha: &lt;/emoji&gt; &lt;/spoiler&gt; Falouws &lt;emoji&gt; :cool: &lt;/emoji&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/members/sugien.2639854/</td>\n",
       "      <td>ASHUAHSIAH, é a GTR normal, a de 1288Mhz. Em stock, ela fica em 1.05v já. Com 1.0v ela funciona tranquilo, mas qualquer coisa abaixo disso no wattman não parece ser aplicado .-. Por enquanto só testei com Dirt 3 e com o kombustor. To baixando TW3 pra testar mais esse undervolt Tu fez undervolt no wattman, ou usou outro programa, tipo o afterburner? Como ficou a sua configuração das fans?</td>\n",
       "      <td>Como padrão já fica 1,05V?! &lt;emoji&gt; :putz: &lt;/emoji&gt; &lt;emoji&gt; :medo: &lt;/emoji&gt; Então tá explicado porque os chips com ASIC alto não conseguem clocks maiores: limitação no aumento do vcore/PL na BIOS, que só vai até +100mV e 50%... Pra superar isso, só fazendo biosmod mesmo. Com 1V funciona tranquilo? Bom demais! &lt;emoji&gt; :megusta: &lt;/emoji&gt; Aqui não deve chegar nem a pau a ficar estável assim &lt;emoji&gt; :feelbad: &lt;/emoji&gt; Depois faz um log de um gameplay de um jogo relativamente pesado usando o GPU-Z, que monto o gráfico e posto aqui &lt;emoji&gt; :joia: &lt;/emoji&gt; Não dá pra reduzir pra menos que isso no Wattman? Acabei de simular 995mV e foi, ele aceitou a mudança, mas não testei na prática por motivos óbvios. Tenta fazer com o Afterburner pra ver se surte algum efeito &lt;emoji&gt; :joia: &lt;/emoji&gt; Fiz o meu processo usando o Wattman mesmo, pois é o único que consegue alterar todos os estados da placa, diferente do Afterburner, que só mexe no Estado 7. As fans eu deixei em auto mesmo, do jeito que veio configurado na BIOS. A placa chega em 70°C, mas a fan gira a, no máximo, 47%; a redução foi muito boa (de 80, com vcore stock, para 47%) e poderia ainda ser maior se eu afrouxasse um pouco o target da temperatura. Mas a 660 TI e a 7950 tem consumo bem parecido, então por isso não deve ter causado tanto impacto; no caso da 7970 GHz, existe um aumento em relação à 7950 perceptível, porém foi como você mesmo disse: o consumo final depende das horas de jogatina por mês. No caso da 290 x 480, a diferença pode não ter se mostrado expressiva pelo fato de tu ter feito overclock nela, o que aumenta consideravelmente seu consumo, fazendo com que o mesmo fique próximo do que a 290 consumia. É, eu gostei dessa correção que a AMD fez, mas bem que ela poderia também ter mantido perfis dinâmicos pra GDDR5 também, ao invés de ficar nessa onda de 300-2000MHz; se ao menos ela deixasse a gente mexer o clock da memória pra cada P State, mas nem isso ela deixa &lt;emoji&gt; :bua: &lt;/emoji&gt; E tipo, aumentar de 150 pra 300 MHz o clock da GPU em relação à minha 270X? Pra que tudo isso se o chip já processa bem 2D com 150MHz? &lt;emoji&gt; :chan: &lt;/emoji&gt; Falouws &lt;emoji&gt; :cool: &lt;/emoji&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/members/l.2408574/</td>\n",
       "      <td>Se essa oscilação é no MGSV a resposta é muito simples, pelo fato do jogo ser travado a 60fps a 480 entra em pequenos períodos de idle rodando o game, simplesmente porque não precisa usar todo o desempenho para manter 60fps e o jogo não deixa ir além disso. Vai acontecer em qualquer game que ela aguente mais de 60fps e esteja travado em 60, isso inclusive ajuda a economizar energia &lt;emoji&gt; :coolface: &lt;/emoji&gt; , como meu monitor é 144Hz isso aqui só acontece no MGSV mesmo &lt;emoji&gt; :mr: &lt;/emoji&gt; .</td>\n",
       "      <td>Ninguém tinha relatado coil whine nas GTR, então achei que não fosse acontecer comigo. Ledo engano... Eu até perguntei antes aqui, no dia que teve 3 caras que receberam RX no tópico: AS PLACAS ESTÃO GERANDO COIL WHINE?? Mas, infelizmente, ninguém respondeu... &lt;emoji&gt; :bua: &lt;/emoji&gt; De qualquer forma, para evitar o estresse de ter que devolver e tal, até porque provavelmente a Pichau não aceitaria a placa de volta só por causa disso, vou deixar como tá mesmo e ir levando até onde der &lt;emoji&gt; :feelbad: &lt;/emoji&gt; Tá ficando cego (ou velho) que nem o &lt;url&gt; forum.adrenaline.com.br @rexBauru &lt;/url&gt; ? &lt;emoji&gt; :haha: &lt;/emoji&gt; &lt;emoji&gt; :haha3: &lt;/emoji&gt; Eu disse que testei o jogo no MGSV, porém não olhei o gráfico dele porque vi que rodou liso feito bunda de bebê, ao contrário do RoTR. Esse sim eu observei o gráfico e colei o mesmo no meu post anterior, e ele apresentou estas oscilações que não eram para ocorrer ao meu ver, já que não travei o Vsync. Eu já suspeitava disso, tendo em vista que o controle do One S é compatível com o Xone original, que não tem Bluetooth. Só resta saber agora quem é que usa a saída P2 do controle pro fone, se é o Bluetooth ou se o receiver WiFi também dá conta de transmitir áudio por ali... Eu ainda acho que comigo será maior, depois que eu fechar todo o case. É por conta disso que meu foco é fazer undervolting agressivo, pra deixar o consumo e as temperaturas em níveis que considero aceitáveis. Só pra zoar, rodei o bench do RoTR com o Power Limit setado em 50% (depois que vi, no Wattman você precisa ativar o controle de temperatura se quiser mexer no PL): &lt;image&gt; A GPU deu um pico pra 1266, mas durante o teste, acho que nem ultrapassou os 800 MHz. O consumo ficou tão baixo que eu achei que estava usando minha 750 TI &lt;emoji&gt; :bwahaha: &lt;/emoji&gt; Claro, no final das contas, ainda precisamos levar em consideração o consumo das memórias, do AUX e do cooler, mas mesmo diante desse \"péssimo cenário\", olha o resultado que tive na performance: &lt;image&gt; E o resultado que tive com a 750 TI, sob as mesmas condições (qualidade máxima, SMAA): &lt;image&gt; Fazendo mais que o dobro de performance média, consumindo um pouco acima dos 75W &lt;emoji&gt; :lol2: &lt;/emoji&gt; Antes que me esqueça, segue abaixo as imagens tiradas do GPU-Z, incluindo o ASIC da minha Polaris 10: &lt;image&gt; &lt;image&gt; &lt;image&gt; Queria acima dos 80%, mas infelizmente não se pode ter tudo né? &lt;emoji&gt; :feelbad: &lt;/emoji&gt; PS: &lt;url&gt; forum.adrenaline.com.br @rexBauru &lt;/url&gt; , tentei por 1400 MHz aqui, mas não rolou... Assim que começo a rodar o bench do RoTR, o jogo trava e eu preciso reiniciar o PC. Nem mesmo colocar o PL em 50% surtiu efeito; pelo visto terei que aumentar o vcore pra poder compensar &lt;emoji&gt; :feelbad: &lt;/emoji&gt; Falouws &lt;emoji&gt; :cool: &lt;/emoji&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/members/_eagleman_.150712/</td>\n",
       "      <td>Não é bem R$ 900, mas de qualquer modo é uma placa que nasceu morta por causa disso (ao menos na Mamute, que não é nenhum sinônimo de preço baixo, mas pelo menos tem um atendimento decente no pós-vendas). Segundo a última atualização que recebi, sim: Última(s) atualização(ões): 09/11/2016 09:29 - CEE NATAL - Natal/RN - Objeto saiu para entrega ao destinatário Pelo visto passar no postinho pra tomar remédio e vacina surtiu algum efeito positivo na rapidez da entrega (apesar que eu achava que ainda poderia ter chegado antes, mas isso já deve ser influência de fim de ano) &lt;emoji&gt; :D &lt;/emoji&gt; Tem gente que ainda está pra receber essa semana, segundo relatos do tópico do For Sale. Portanto, você não deve ser a última, pelo menos não ainda. De qualquer modo, lembre da sua faculdade e das provas, você precisa se concentrar nelas que é o mais importante agora &lt;emoji&gt; :true: &lt;/emoji&gt; A diferença no TDP nem sempre se traduz em consumo elétrico, no caso, achar que a 470 vai consumir um pouco mais que a 260X, pois não irá: &lt;spoiler&gt; Spoiler: Consumo elétrico R7 260X &lt;image&gt; &lt;image&gt; &lt;image&gt; &lt;/spoiler&gt; Lembrando que essa HIS roda com os mesmos clocks do modelo de referência, portanto deveria apresentar mesmo consumo. Agora, no caso da 470: &lt;spoiler&gt; Spoiler: Consumo elétrico RX 470 &lt;image&gt; &lt;image&gt; &lt;image&gt; &lt;/spoiler&gt; Mesmo no consumo típico ingame, o consumo aumentou quase 40W, comparando os modelos com clocks de referência, o que já pode ser considerado razoável (e completamente justificável, tendo em vista as diferenças significativas existentes entre os chips e na forma como ele trabalha no que diz respeito à gestão elétrica). Porém, são quase 40W a mais pra entregar um ônibus espacial de performance adicional, então tá valendo &lt;emoji&gt; :joia: &lt;/emoji&gt; O TDP nesse caso serve mais pra traduzir que o chip tão pequeno quanto aquele Bonaire é capaz de dissipar quase a mesma quantidade de calor que o Polaris 10 e que, portanto, precisa de um cooler razoável pra deixar a temperatura num nível decente. Olha a diferença entre o modelo de referência e o da HIS, que é dual fan: &lt;image&gt; &lt;image&gt; Isso apenas mostra que qualquer chip pode esquentar muito, basta usar um cooler porcaria &lt;emoji&gt; :true: &lt;/emoji&gt; O problema é que existe algo que está impedindo a placa de trabalhar com seu boost padrão e esse alguém pode ser a fonte (que, até o momento, foi capaz de segurar outras configurações, inclusive com a 480, então talvez seja defeito nela), pode ser a VGA (que pode ter vindo zoada de fábrica, ao ponto de não aguentar os clocks padrão, vai saber...), ou pode ser as opções de energia na placa-mãe contra proteção de surtos elétricos. Eu não tenho como precisar, pois eu uso uma plataforma de quase 7 anos de vida, então estou bem desatualizado das novidades inseridas nas plataformas atuais &lt;emoji&gt; :feelbad: &lt;/emoji&gt; &lt;mediaembed&gt; youtube &lt;/mediaembed&gt; Não são mesmo: basta ver que a Kabum recentemente subiu os preços dos seus produtos para nos preparar para sua Black FRAUDE... &lt;emoji&gt; :kidding: &lt;/emoji&gt; &lt;emoji&gt; :facepalm: &lt;/emoji&gt; Falouws &lt;emoji&gt; :cool: &lt;/emoji&gt;</td>\n",
       "      <td>Então, estou respirando na faculdade já e desejo do fundo do coração que ela chegue até sexta, porque amanhã fico oficialmente de férias. Agora eu queria que minha placa fosse para Saúde também, porque veio direto para cá em Salvador e nada de chegar, enquanto a sua foi para São Paulo e Recife e já chegou. De Porto Alegre e ainda não chegou? É, Trump eleito pode significar aumento do dólar rapinho, mas acho que dá para esperar até o dia 11. Agora para quem ainda não comprou a placa, fica a dica de se puder comprar logo, porque vai que aumente.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/members/wellyngtonbb.2315752/</td>\n",
       "      <td>Boa noite galera, adiquiri um kit phenom x4 960t, 2x4GB ddr3 1600Mhz. Destravei os 2 núcleos dele pela bios, mas, depois que reiniciei, ela trava na bios e não sai de lah, ou fica carregando em um nome \" Loading Operating System\" ... Alguém já passou por isso ao destravar o Phenom x4 960t? Obrigado a todos por lerem.</td>\n",
       "      <td>Possibilidades: 1 - Algum núcleo destravado ( ou os dois ) está com defeito e não é utilizável. 2 - Você aumentou o vcore do processador ? o 960t usa por padrão 1.325v. Experimente 1.375/1.4v que é o vcore dos X6 3 - A bios permite desativar ou escolher quais núcleos estão ativos ? experimente desabilitar o core 4 ou 5 se for possível</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"] + examples[\"reply\"],\n",
    "                       padding=\"max_length\", truncation=True, max_length=64)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    num_proc=8,\n",
    "    batched=True,\n",
    "    batch_size=128,\n",
    "    remove_columns=[\"id\", \"text\", \"reply\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train transformerslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=2000,\n",
    "    num_train_epochs=20,\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(50000)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 50000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 125000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84791' max='125000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 84791/125000 3:09:18 < 1:29:46, 7.46 it/s, Epoch 13.57/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.738600</td>\n",
       "      <td>3.050401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>2.933991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>2.865727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.591600</td>\n",
       "      <td>2.845868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>2.820881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>2.820131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>2.840756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.465100</td>\n",
       "      <td>2.801399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>2.833448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>2.883101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>2.852505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>2.857430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>2.881670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>2.873842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-clm/checkpoint-500\n",
      "Configuration saved in test-clm/checkpoint-500/config.json\n",
      "Model weights saved in test-clm/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-1000\n",
      "Configuration saved in test-clm/checkpoint-1000/config.json\n",
      "Model weights saved in test-clm/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-1500\n",
      "Configuration saved in test-clm/checkpoint-1500/config.json\n",
      "Model weights saved in test-clm/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-2000\n",
      "Configuration saved in test-clm/checkpoint-2000/config.json\n",
      "Model weights saved in test-clm/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-2500\n",
      "Configuration saved in test-clm/checkpoint-2500/config.json\n",
      "Model weights saved in test-clm/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-3000\n",
      "Configuration saved in test-clm/checkpoint-3000/config.json\n",
      "Model weights saved in test-clm/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-3500\n",
      "Configuration saved in test-clm/checkpoint-3500/config.json\n",
      "Model weights saved in test-clm/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-4000\n",
      "Configuration saved in test-clm/checkpoint-4000/config.json\n",
      "Model weights saved in test-clm/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-4500\n",
      "Configuration saved in test-clm/checkpoint-4500/config.json\n",
      "Model weights saved in test-clm/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-5000\n",
      "Configuration saved in test-clm/checkpoint-5000/config.json\n",
      "Model weights saved in test-clm/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-5500\n",
      "Configuration saved in test-clm/checkpoint-5500/config.json\n",
      "Model weights saved in test-clm/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-6000\n",
      "Configuration saved in test-clm/checkpoint-6000/config.json\n",
      "Model weights saved in test-clm/checkpoint-6000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-6500\n",
      "Configuration saved in test-clm/checkpoint-6500/config.json\n",
      "Model weights saved in test-clm/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-7000\n",
      "Configuration saved in test-clm/checkpoint-7000/config.json\n",
      "Model weights saved in test-clm/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-7500\n",
      "Configuration saved in test-clm/checkpoint-7500/config.json\n",
      "Model weights saved in test-clm/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-8000\n",
      "Configuration saved in test-clm/checkpoint-8000/config.json\n",
      "Model weights saved in test-clm/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-8500\n",
      "Configuration saved in test-clm/checkpoint-8500/config.json\n",
      "Model weights saved in test-clm/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-9000\n",
      "Configuration saved in test-clm/checkpoint-9000/config.json\n",
      "Model weights saved in test-clm/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-9500\n",
      "Configuration saved in test-clm/checkpoint-9500/config.json\n",
      "Model weights saved in test-clm/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-10000\n",
      "Configuration saved in test-clm/checkpoint-10000/config.json\n",
      "Model weights saved in test-clm/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-10500\n",
      "Configuration saved in test-clm/checkpoint-10500/config.json\n",
      "Model weights saved in test-clm/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-11000\n",
      "Configuration saved in test-clm/checkpoint-11000/config.json\n",
      "Model weights saved in test-clm/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-11500\n",
      "Configuration saved in test-clm/checkpoint-11500/config.json\n",
      "Model weights saved in test-clm/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-12000\n",
      "Configuration saved in test-clm/checkpoint-12000/config.json\n",
      "Model weights saved in test-clm/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-12500\n",
      "Configuration saved in test-clm/checkpoint-12500/config.json\n",
      "Model weights saved in test-clm/checkpoint-12500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-13000\n",
      "Configuration saved in test-clm/checkpoint-13000/config.json\n",
      "Model weights saved in test-clm/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-13500\n",
      "Configuration saved in test-clm/checkpoint-13500/config.json\n",
      "Model weights saved in test-clm/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-14000\n",
      "Configuration saved in test-clm/checkpoint-14000/config.json\n",
      "Model weights saved in test-clm/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-14500\n",
      "Configuration saved in test-clm/checkpoint-14500/config.json\n",
      "Model weights saved in test-clm/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-15000\n",
      "Configuration saved in test-clm/checkpoint-15000/config.json\n",
      "Model weights saved in test-clm/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-15500\n",
      "Configuration saved in test-clm/checkpoint-15500/config.json\n",
      "Model weights saved in test-clm/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-16000\n",
      "Configuration saved in test-clm/checkpoint-16000/config.json\n",
      "Model weights saved in test-clm/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-16500\n",
      "Configuration saved in test-clm/checkpoint-16500/config.json\n",
      "Model weights saved in test-clm/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-17000\n",
      "Configuration saved in test-clm/checkpoint-17000/config.json\n",
      "Model weights saved in test-clm/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-17500\n",
      "Configuration saved in test-clm/checkpoint-17500/config.json\n",
      "Model weights saved in test-clm/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-18000\n",
      "Configuration saved in test-clm/checkpoint-18000/config.json\n",
      "Model weights saved in test-clm/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-18500\n",
      "Configuration saved in test-clm/checkpoint-18500/config.json\n",
      "Model weights saved in test-clm/checkpoint-18500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-19000\n",
      "Configuration saved in test-clm/checkpoint-19000/config.json\n",
      "Model weights saved in test-clm/checkpoint-19000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-19500\n",
      "Configuration saved in test-clm/checkpoint-19500/config.json\n",
      "Model weights saved in test-clm/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-20000\n",
      "Configuration saved in test-clm/checkpoint-20000/config.json\n",
      "Model weights saved in test-clm/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-20500\n",
      "Configuration saved in test-clm/checkpoint-20500/config.json\n",
      "Model weights saved in test-clm/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-21000\n",
      "Configuration saved in test-clm/checkpoint-21000/config.json\n",
      "Model weights saved in test-clm/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-21500\n",
      "Configuration saved in test-clm/checkpoint-21500/config.json\n",
      "Model weights saved in test-clm/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-22000\n",
      "Configuration saved in test-clm/checkpoint-22000/config.json\n",
      "Model weights saved in test-clm/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-22500\n",
      "Configuration saved in test-clm/checkpoint-22500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test-clm/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-23000\n",
      "Configuration saved in test-clm/checkpoint-23000/config.json\n",
      "Model weights saved in test-clm/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-23500\n",
      "Configuration saved in test-clm/checkpoint-23500/config.json\n",
      "Model weights saved in test-clm/checkpoint-23500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-24000\n",
      "Configuration saved in test-clm/checkpoint-24000/config.json\n",
      "Model weights saved in test-clm/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-24500\n",
      "Configuration saved in test-clm/checkpoint-24500/config.json\n",
      "Model weights saved in test-clm/checkpoint-24500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-25000\n",
      "Configuration saved in test-clm/checkpoint-25000/config.json\n",
      "Model weights saved in test-clm/checkpoint-25000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-25500\n",
      "Configuration saved in test-clm/checkpoint-25500/config.json\n",
      "Model weights saved in test-clm/checkpoint-25500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-26000\n",
      "Configuration saved in test-clm/checkpoint-26000/config.json\n",
      "Model weights saved in test-clm/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-26500\n",
      "Configuration saved in test-clm/checkpoint-26500/config.json\n",
      "Model weights saved in test-clm/checkpoint-26500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-27000\n",
      "Configuration saved in test-clm/checkpoint-27000/config.json\n",
      "Model weights saved in test-clm/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-27500\n",
      "Configuration saved in test-clm/checkpoint-27500/config.json\n",
      "Model weights saved in test-clm/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-28000\n",
      "Configuration saved in test-clm/checkpoint-28000/config.json\n",
      "Model weights saved in test-clm/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-28500\n",
      "Configuration saved in test-clm/checkpoint-28500/config.json\n",
      "Model weights saved in test-clm/checkpoint-28500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-29000\n",
      "Configuration saved in test-clm/checkpoint-29000/config.json\n",
      "Model weights saved in test-clm/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-29500\n",
      "Configuration saved in test-clm/checkpoint-29500/config.json\n",
      "Model weights saved in test-clm/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-30000\n",
      "Configuration saved in test-clm/checkpoint-30000/config.json\n",
      "Model weights saved in test-clm/checkpoint-30000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-30500\n",
      "Configuration saved in test-clm/checkpoint-30500/config.json\n",
      "Model weights saved in test-clm/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-31000\n",
      "Configuration saved in test-clm/checkpoint-31000/config.json\n",
      "Model weights saved in test-clm/checkpoint-31000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-31500\n",
      "Configuration saved in test-clm/checkpoint-31500/config.json\n",
      "Model weights saved in test-clm/checkpoint-31500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-32000\n",
      "Configuration saved in test-clm/checkpoint-32000/config.json\n",
      "Model weights saved in test-clm/checkpoint-32000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-32500\n",
      "Configuration saved in test-clm/checkpoint-32500/config.json\n",
      "Model weights saved in test-clm/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-33000\n",
      "Configuration saved in test-clm/checkpoint-33000/config.json\n",
      "Model weights saved in test-clm/checkpoint-33000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-33500\n",
      "Configuration saved in test-clm/checkpoint-33500/config.json\n",
      "Model weights saved in test-clm/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-34000\n",
      "Configuration saved in test-clm/checkpoint-34000/config.json\n",
      "Model weights saved in test-clm/checkpoint-34000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-34500\n",
      "Configuration saved in test-clm/checkpoint-34500/config.json\n",
      "Model weights saved in test-clm/checkpoint-34500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-35000\n",
      "Configuration saved in test-clm/checkpoint-35000/config.json\n",
      "Model weights saved in test-clm/checkpoint-35000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-35500\n",
      "Configuration saved in test-clm/checkpoint-35500/config.json\n",
      "Model weights saved in test-clm/checkpoint-35500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-36000\n",
      "Configuration saved in test-clm/checkpoint-36000/config.json\n",
      "Model weights saved in test-clm/checkpoint-36000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-36500\n",
      "Configuration saved in test-clm/checkpoint-36500/config.json\n",
      "Model weights saved in test-clm/checkpoint-36500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-37000\n",
      "Configuration saved in test-clm/checkpoint-37000/config.json\n",
      "Model weights saved in test-clm/checkpoint-37000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-37500\n",
      "Configuration saved in test-clm/checkpoint-37500/config.json\n",
      "Model weights saved in test-clm/checkpoint-37500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-38000\n",
      "Configuration saved in test-clm/checkpoint-38000/config.json\n",
      "Model weights saved in test-clm/checkpoint-38000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-38500\n",
      "Configuration saved in test-clm/checkpoint-38500/config.json\n",
      "Model weights saved in test-clm/checkpoint-38500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-39000\n",
      "Configuration saved in test-clm/checkpoint-39000/config.json\n",
      "Model weights saved in test-clm/checkpoint-39000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-39500\n",
      "Configuration saved in test-clm/checkpoint-39500/config.json\n",
      "Model weights saved in test-clm/checkpoint-39500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-40000\n",
      "Configuration saved in test-clm/checkpoint-40000/config.json\n",
      "Model weights saved in test-clm/checkpoint-40000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-40500\n",
      "Configuration saved in test-clm/checkpoint-40500/config.json\n",
      "Model weights saved in test-clm/checkpoint-40500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-41000\n",
      "Configuration saved in test-clm/checkpoint-41000/config.json\n",
      "Model weights saved in test-clm/checkpoint-41000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-41500\n",
      "Configuration saved in test-clm/checkpoint-41500/config.json\n",
      "Model weights saved in test-clm/checkpoint-41500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-42000\n",
      "Configuration saved in test-clm/checkpoint-42000/config.json\n",
      "Model weights saved in test-clm/checkpoint-42000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-42500\n",
      "Configuration saved in test-clm/checkpoint-42500/config.json\n",
      "Model weights saved in test-clm/checkpoint-42500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-43000\n",
      "Configuration saved in test-clm/checkpoint-43000/config.json\n",
      "Model weights saved in test-clm/checkpoint-43000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-43500\n",
      "Configuration saved in test-clm/checkpoint-43500/config.json\n",
      "Model weights saved in test-clm/checkpoint-43500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-44000\n",
      "Configuration saved in test-clm/checkpoint-44000/config.json\n",
      "Model weights saved in test-clm/checkpoint-44000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-44500\n",
      "Configuration saved in test-clm/checkpoint-44500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test-clm/checkpoint-44500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-45000\n",
      "Configuration saved in test-clm/checkpoint-45000/config.json\n",
      "Model weights saved in test-clm/checkpoint-45000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-45500\n",
      "Configuration saved in test-clm/checkpoint-45500/config.json\n",
      "Model weights saved in test-clm/checkpoint-45500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-46000\n",
      "Configuration saved in test-clm/checkpoint-46000/config.json\n",
      "Model weights saved in test-clm/checkpoint-46000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-46500\n",
      "Configuration saved in test-clm/checkpoint-46500/config.json\n",
      "Model weights saved in test-clm/checkpoint-46500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-47000\n",
      "Configuration saved in test-clm/checkpoint-47000/config.json\n",
      "Model weights saved in test-clm/checkpoint-47000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-47500\n",
      "Configuration saved in test-clm/checkpoint-47500/config.json\n",
      "Model weights saved in test-clm/checkpoint-47500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-48000\n",
      "Configuration saved in test-clm/checkpoint-48000/config.json\n",
      "Model weights saved in test-clm/checkpoint-48000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-48500\n",
      "Configuration saved in test-clm/checkpoint-48500/config.json\n",
      "Model weights saved in test-clm/checkpoint-48500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-49000\n",
      "Configuration saved in test-clm/checkpoint-49000/config.json\n",
      "Model weights saved in test-clm/checkpoint-49000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-49500\n",
      "Configuration saved in test-clm/checkpoint-49500/config.json\n",
      "Model weights saved in test-clm/checkpoint-49500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-50000\n",
      "Configuration saved in test-clm/checkpoint-50000/config.json\n",
      "Model weights saved in test-clm/checkpoint-50000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-50500\n",
      "Configuration saved in test-clm/checkpoint-50500/config.json\n",
      "Model weights saved in test-clm/checkpoint-50500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-51000\n",
      "Configuration saved in test-clm/checkpoint-51000/config.json\n",
      "Model weights saved in test-clm/checkpoint-51000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-51500\n",
      "Configuration saved in test-clm/checkpoint-51500/config.json\n",
      "Model weights saved in test-clm/checkpoint-51500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-52000\n",
      "Configuration saved in test-clm/checkpoint-52000/config.json\n",
      "Model weights saved in test-clm/checkpoint-52000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-52500\n",
      "Configuration saved in test-clm/checkpoint-52500/config.json\n",
      "Model weights saved in test-clm/checkpoint-52500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-53000\n",
      "Configuration saved in test-clm/checkpoint-53000/config.json\n",
      "Model weights saved in test-clm/checkpoint-53000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-53500\n",
      "Configuration saved in test-clm/checkpoint-53500/config.json\n",
      "Model weights saved in test-clm/checkpoint-53500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-54000\n",
      "Configuration saved in test-clm/checkpoint-54000/config.json\n",
      "Model weights saved in test-clm/checkpoint-54000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-54500\n",
      "Configuration saved in test-clm/checkpoint-54500/config.json\n",
      "Model weights saved in test-clm/checkpoint-54500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-55000\n",
      "Configuration saved in test-clm/checkpoint-55000/config.json\n",
      "Model weights saved in test-clm/checkpoint-55000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-55500\n",
      "Configuration saved in test-clm/checkpoint-55500/config.json\n",
      "Model weights saved in test-clm/checkpoint-55500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-56000\n",
      "Configuration saved in test-clm/checkpoint-56000/config.json\n",
      "Model weights saved in test-clm/checkpoint-56000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-56500\n",
      "Configuration saved in test-clm/checkpoint-56500/config.json\n",
      "Model weights saved in test-clm/checkpoint-56500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-57000\n",
      "Configuration saved in test-clm/checkpoint-57000/config.json\n",
      "Model weights saved in test-clm/checkpoint-57000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-57500\n",
      "Configuration saved in test-clm/checkpoint-57500/config.json\n",
      "Model weights saved in test-clm/checkpoint-57500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-58000\n",
      "Configuration saved in test-clm/checkpoint-58000/config.json\n",
      "Model weights saved in test-clm/checkpoint-58000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-58500\n",
      "Configuration saved in test-clm/checkpoint-58500/config.json\n",
      "Model weights saved in test-clm/checkpoint-58500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-59000\n",
      "Configuration saved in test-clm/checkpoint-59000/config.json\n",
      "Model weights saved in test-clm/checkpoint-59000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-59500\n",
      "Configuration saved in test-clm/checkpoint-59500/config.json\n",
      "Model weights saved in test-clm/checkpoint-59500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-60000\n",
      "Configuration saved in test-clm/checkpoint-60000/config.json\n",
      "Model weights saved in test-clm/checkpoint-60000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-60500\n",
      "Configuration saved in test-clm/checkpoint-60500/config.json\n",
      "Model weights saved in test-clm/checkpoint-60500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-61000\n",
      "Configuration saved in test-clm/checkpoint-61000/config.json\n",
      "Model weights saved in test-clm/checkpoint-61000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-61500\n",
      "Configuration saved in test-clm/checkpoint-61500/config.json\n",
      "Model weights saved in test-clm/checkpoint-61500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-62000\n",
      "Configuration saved in test-clm/checkpoint-62000/config.json\n",
      "Model weights saved in test-clm/checkpoint-62000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-62500\n",
      "Configuration saved in test-clm/checkpoint-62500/config.json\n",
      "Model weights saved in test-clm/checkpoint-62500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-63000\n",
      "Configuration saved in test-clm/checkpoint-63000/config.json\n",
      "Model weights saved in test-clm/checkpoint-63000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-63500\n",
      "Configuration saved in test-clm/checkpoint-63500/config.json\n",
      "Model weights saved in test-clm/checkpoint-63500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-64000\n",
      "Configuration saved in test-clm/checkpoint-64000/config.json\n",
      "Model weights saved in test-clm/checkpoint-64000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-64500\n",
      "Configuration saved in test-clm/checkpoint-64500/config.json\n",
      "Model weights saved in test-clm/checkpoint-64500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-65000\n",
      "Configuration saved in test-clm/checkpoint-65000/config.json\n",
      "Model weights saved in test-clm/checkpoint-65000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-65500\n",
      "Configuration saved in test-clm/checkpoint-65500/config.json\n",
      "Model weights saved in test-clm/checkpoint-65500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-66000\n",
      "Configuration saved in test-clm/checkpoint-66000/config.json\n",
      "Model weights saved in test-clm/checkpoint-66000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-66500\n",
      "Configuration saved in test-clm/checkpoint-66500/config.json\n",
      "Model weights saved in test-clm/checkpoint-66500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-clm/checkpoint-67000\n",
      "Configuration saved in test-clm/checkpoint-67000/config.json\n",
      "Model weights saved in test-clm/checkpoint-67000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-67500\n",
      "Configuration saved in test-clm/checkpoint-67500/config.json\n",
      "Model weights saved in test-clm/checkpoint-67500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-68000\n",
      "Configuration saved in test-clm/checkpoint-68000/config.json\n",
      "Model weights saved in test-clm/checkpoint-68000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-68500\n",
      "Configuration saved in test-clm/checkpoint-68500/config.json\n",
      "Model weights saved in test-clm/checkpoint-68500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-69000\n",
      "Configuration saved in test-clm/checkpoint-69000/config.json\n",
      "Model weights saved in test-clm/checkpoint-69000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-69500\n",
      "Configuration saved in test-clm/checkpoint-69500/config.json\n",
      "Model weights saved in test-clm/checkpoint-69500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-70000\n",
      "Configuration saved in test-clm/checkpoint-70000/config.json\n",
      "Model weights saved in test-clm/checkpoint-70000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-70500\n",
      "Configuration saved in test-clm/checkpoint-70500/config.json\n",
      "Model weights saved in test-clm/checkpoint-70500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-71000\n",
      "Configuration saved in test-clm/checkpoint-71000/config.json\n",
      "Model weights saved in test-clm/checkpoint-71000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-71500\n",
      "Configuration saved in test-clm/checkpoint-71500/config.json\n",
      "Model weights saved in test-clm/checkpoint-71500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-72000\n",
      "Configuration saved in test-clm/checkpoint-72000/config.json\n",
      "Model weights saved in test-clm/checkpoint-72000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-72500\n",
      "Configuration saved in test-clm/checkpoint-72500/config.json\n",
      "Model weights saved in test-clm/checkpoint-72500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-73000\n",
      "Configuration saved in test-clm/checkpoint-73000/config.json\n",
      "Model weights saved in test-clm/checkpoint-73000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-73500\n",
      "Configuration saved in test-clm/checkpoint-73500/config.json\n",
      "Model weights saved in test-clm/checkpoint-73500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-74000\n",
      "Configuration saved in test-clm/checkpoint-74000/config.json\n",
      "Model weights saved in test-clm/checkpoint-74000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-74500\n",
      "Configuration saved in test-clm/checkpoint-74500/config.json\n",
      "Model weights saved in test-clm/checkpoint-74500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-75000\n",
      "Configuration saved in test-clm/checkpoint-75000/config.json\n",
      "Model weights saved in test-clm/checkpoint-75000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-75500\n",
      "Configuration saved in test-clm/checkpoint-75500/config.json\n",
      "Model weights saved in test-clm/checkpoint-75500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-76000\n",
      "Configuration saved in test-clm/checkpoint-76000/config.json\n",
      "Model weights saved in test-clm/checkpoint-76000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-76500\n",
      "Configuration saved in test-clm/checkpoint-76500/config.json\n",
      "Model weights saved in test-clm/checkpoint-76500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-77000\n",
      "Configuration saved in test-clm/checkpoint-77000/config.json\n",
      "Model weights saved in test-clm/checkpoint-77000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-77500\n",
      "Configuration saved in test-clm/checkpoint-77500/config.json\n",
      "Model weights saved in test-clm/checkpoint-77500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-78000\n",
      "Configuration saved in test-clm/checkpoint-78000/config.json\n",
      "Model weights saved in test-clm/checkpoint-78000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-78500\n",
      "Configuration saved in test-clm/checkpoint-78500/config.json\n",
      "Model weights saved in test-clm/checkpoint-78500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-79000\n",
      "Configuration saved in test-clm/checkpoint-79000/config.json\n",
      "Model weights saved in test-clm/checkpoint-79000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-79500\n",
      "Configuration saved in test-clm/checkpoint-79500/config.json\n",
      "Model weights saved in test-clm/checkpoint-79500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-80000\n",
      "Configuration saved in test-clm/checkpoint-80000/config.json\n",
      "Model weights saved in test-clm/checkpoint-80000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-80500\n",
      "Configuration saved in test-clm/checkpoint-80500/config.json\n",
      "Model weights saved in test-clm/checkpoint-80500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-81000\n",
      "Configuration saved in test-clm/checkpoint-81000/config.json\n",
      "Model weights saved in test-clm/checkpoint-81000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test-clm/checkpoint-81500\n",
      "Configuration saved in test-clm/checkpoint-81500/config.json\n",
      "Model weights saved in test-clm/checkpoint-81500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-82000\n",
      "Configuration saved in test-clm/checkpoint-82000/config.json\n",
      "Model weights saved in test-clm/checkpoint-82000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-82500\n",
      "Configuration saved in test-clm/checkpoint-82500/config.json\n",
      "Model weights saved in test-clm/checkpoint-82500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-83000\n",
      "Configuration saved in test-clm/checkpoint-83000/config.json\n",
      "Model weights saved in test-clm/checkpoint-83000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-83500\n",
      "Configuration saved in test-clm/checkpoint-83500/config.json\n",
      "Model weights saved in test-clm/checkpoint-83500/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-84000\n",
      "Configuration saved in test-clm/checkpoint-84000/config.json\n",
      "Model weights saved in test-clm/checkpoint-84000/pytorch_model.bin\n",
      "Saving model checkpoint to test-clm/checkpoint-84500\n",
      "Configuration saved in test-clm/checkpoint-84500/config.json\n",
      "Model weights saved in test-clm/checkpoint-84500/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5643/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/UBAR-MultiWOZ/venv/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1306\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                             \u001b[0;31m# Revert to normal clipping otherwise, handling Apex or full precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m                             nn.utils.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   1309\u001b[0m                                 \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m                                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/UBAR-MultiWOZ/venv/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 17.70\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx7-DIBNn72W"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN1m54QreJ0N",
    "outputId": "b6751277-5fe4-4ded-d77d-5459ba8953bf"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10000)), batch_size=8)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000)), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DLKGGvjAxLt"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sXyIuIv_0yjX",
    "outputId": "8d850663-0031-40f1-f05d-bee49af5ada0"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=2000,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YolTBooPChG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm.notebook.tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    print(tokenizer.decode(pred[0].tolist()))\n",
    "    print(tokenizer.decode(batch[\"labels\"][0]))\n",
    "    metric.compute(predictions=pred, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PortugueseUBAR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02bf53ed5c80455e94f21ef214c60826": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0569c7d0bafa45768a4644d15c7b29ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "05e59f27260d4429a582864c07708e3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_792e327d669f4fd08f00fbcbd9d755d0",
      "placeholder": "​",
      "style": "IPY_MODEL_f1a3c0b3150344c8ab5a61b52b37eaee",
      "value": " 0/? [00:00&lt;?, ? tables/s]"
     }
    },
    "1c5da9b0f1a647dea4db421ee0c94061": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20bafd4d079a462389be362fe4f77fef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24c9a9c556d84476bcae6312a877d83e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28a3aa4c8ab5415f98da29787102047b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "32d5da014a2848978bcd04b1a09d2d19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a1f579f6c4a4b19bf06cb34c180ed34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "4c8178c8ad4b41ce8ace8723e9a3a2b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f91522c36a2454897ba31fd69e82502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "587cd0d9e4ac45568f8f0ca35508b655": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f91522c36a2454897ba31fd69e82502",
      "placeholder": "​",
      "style": "IPY_MODEL_4c8178c8ad4b41ce8ace8723e9a3a2b5",
      "value": ""
     }
    },
    "5afe74e1c35f4b89b9cced0f6aa6c953": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "63208b6df2b1475d97bbe7d742997605": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24c9a9c556d84476bcae6312a877d83e",
      "placeholder": "​",
      "style": "IPY_MODEL_b717d1bdc06b4d34b169f6f839989aea",
      "value": ""
     }
    },
    "674bc84a11824f8db36f7b23c281a648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28a3aa4c8ab5415f98da29787102047b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5afe74e1c35f4b89b9cced0f6aa6c953",
      "value": 1
     }
    },
    "709e33fcc2c74a00a25cd7610b39a09a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a1f579f6c4a4b19bf06cb34c180ed34",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf7e64d9ae7d4afd939495bb964074fd",
      "value": 1
     }
    },
    "792e327d669f4fd08f00fbcbd9d755d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8224bf4f7b014e1eab1e5b1e7e45e834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02bf53ed5c80455e94f21ef214c60826",
      "placeholder": "​",
      "style": "IPY_MODEL_0569c7d0bafa45768a4644d15c7b29ff",
      "value": " 0/? [00:00&lt;?, ? tables/s]"
     }
    },
    "85ee96f7091a475c8a11272f7b9cf498": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "add8441fe9f444a2b397a74fa0aeb66a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1dd47c3ddce4796b927b9350a8bba6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63208b6df2b1475d97bbe7d742997605",
       "IPY_MODEL_709e33fcc2c74a00a25cd7610b39a09a",
       "IPY_MODEL_05e59f27260d4429a582864c07708e3a"
      ],
      "layout": "IPY_MODEL_20bafd4d079a462389be362fe4f77fef"
     }
    },
    "b6f84b4f28ae4cc9be78305b8ac7fd8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85ee96f7091a475c8a11272f7b9cf498",
      "placeholder": "​",
      "style": "IPY_MODEL_add8441fe9f444a2b397a74fa0aeb66a",
      "value": ""
     }
    },
    "b717d1bdc06b4d34b169f6f839989aea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b76352c261364a75bb8aa80c86cbff95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14c5ba9fb5743018126ad27d730e07c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6f84b4f28ae4cc9be78305b8ac7fd8d",
       "IPY_MODEL_d369c447782c4dbba1967fd91dd0ba14",
       "IPY_MODEL_8224bf4f7b014e1eab1e5b1e7e45e834"
      ],
      "layout": "IPY_MODEL_1c5da9b0f1a647dea4db421ee0c94061"
     }
    },
    "cf7e64d9ae7d4afd939495bb964074fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d369c447782c4dbba1967fd91dd0ba14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed729d1efe904f808a8ae206b30e5750",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32d5da014a2848978bcd04b1a09d2d19",
      "value": 1
     }
    },
    "d7bfc1d2e389423280ef863cb55a64bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_587cd0d9e4ac45568f8f0ca35508b655",
       "IPY_MODEL_674bc84a11824f8db36f7b23c281a648",
       "IPY_MODEL_f4ac5634ed7242c1a03cc4626f09fc20"
      ],
      "layout": "IPY_MODEL_b76352c261364a75bb8aa80c86cbff95"
     }
    },
    "ed729d1efe904f808a8ae206b30e5750": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "ee3257d9c73140c99b08d0cc8dc5c789": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1a3c0b3150344c8ab5a61b52b37eaee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4ac5634ed7242c1a03cc4626f09fc20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee3257d9c73140c99b08d0cc8dc5c789",
      "placeholder": "​",
      "style": "IPY_MODEL_f6534287680e4b39a4d5ac56f64c57b5",
      "value": " 957/? [00:02&lt;00:00, 477.28 tables/s]"
     }
    },
    "f6534287680e4b39a4d5ac56f64c57b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
